Metadata-Version: 2.4
Name: infinity_emb
Version: 0.0.77
Summary: Infinity is a high-throughput, low-latency REST API for serving text-embeddings, reranking models and clip.
License: MIT
Keywords: vector,embedding,neural,search,sentence-transformers
Author: michaelfeil
Author-email: noreply@michaelfeil.eu
Requires-Python: >=3.9,<3.14
Classifier: License :: OSI Approved :: MIT License
Classifier: Programming Language :: Python :: 3
Classifier: Programming Language :: Python :: 3.9
Classifier: Programming Language :: Python :: 3.10
Classifier: Programming Language :: Python :: 3.11
Classifier: Programming Language :: Python :: 3.12
Classifier: Programming Language :: Python :: 3.13
Provides-Extra: all
Provides-Extra: audio
Provides-Extra: cache
Provides-Extra: chunking
Provides-Extra: ct2
Provides-Extra: einops
Provides-Extra: grpc
Provides-Extra: logging
Provides-Extra: onnxruntime-gpu
Provides-Extra: optimum
Provides-Extra: server
Provides-Extra: telemetry
Provides-Extra: tensorrt
Provides-Extra: torch
Provides-Extra: tuning
Provides-Extra: vision
Requires-Dist: chonkie (>=1.0.0) ; extra == "chunking" or extra == "all"
Requires-Dist: colpali-engine (>=0.3.8,<0.4.0) ; extra == "vision" or extra == "all"
Requires-Dist: ctranslate2 (>=4.0.0) ; extra == "ct2" or extra == "all"
Requires-Dist: diskcache ; extra == "cache" or extra == "all"
Requires-Dist: einops ; extra == "einops" or extra == "all"
Requires-Dist: fastapi (>=0.103.2) ; extra == "server" or extra == "all"
Requires-Dist: grpcio (>=1.60.0) ; extra == "grpc" or extra == "server" or extra == "all"
Requires-Dist: grpcio-health-checking (>=1.60.0) ; extra == "grpc" or extra == "server" or extra == "all"
Requires-Dist: grpcio-reflection (>=1.60.0) ; extra == "grpc" or extra == "server" or extra == "all"
Requires-Dist: huggingface_hub (>=0.32.0)
Requires-Dist: numpy (>=1.20.0,<2)
Requires-Dist: onnxruntime-gpu (==1.19.*) ; extra == "onnxruntime-gpu"
Requires-Dist: opentelemetry-api (>=1.24.0,<2.0.0) ; extra == "telemetry" or extra == "server"
Requires-Dist: opentelemetry-exporter-otlp (>=1.24.0,<2.0.0) ; extra == "telemetry" or extra == "server"
Requires-Dist: opentelemetry-instrumentation-fastapi (>=0.45b0,<0.46) ; extra == "telemetry" or extra == "server"
Requires-Dist: opentelemetry-sdk (>=1.24.0,<2.0.0) ; extra == "telemetry" or extra == "server"
Requires-Dist: optimum[onnxruntime] (>=1.24.0) ; extra == "optimum" or extra == "all"
Requires-Dist: optuna (>=3.0.0) ; extra == "tuning"
Requires-Dist: orjson (>=3.9.8,!=3.10.0) ; extra == "server" or extra == "all"
Requires-Dist: packaging (>=21.0)
Requires-Dist: pillow ; extra == "vision" or extra == "all"
Requires-Dist: posthog ; extra == "server" or extra == "all"
Requires-Dist: prometheus-fastapi-instrumentator (>=6.1.0) ; extra == "server" or extra == "all"
Requires-Dist: pydantic (>=2.4.0,<3) ; extra == "server" or extra == "all"
Requires-Dist: python-json-logger (>=2.0.0) ; extra == "logging" or extra == "server"
Requires-Dist: rich (>=13,<14) ; extra == "logging" or extra == "server" or extra == "all"
Requires-Dist: sentence-transformers (>=3.0.1,<4.0.0) ; extra == "ct2" or extra == "torch" or extra == "all"
Requires-Dist: soundfile (>=0.12.1,<0.13.0) ; extra == "audio" or extra == "all"
Requires-Dist: tensorrt (>=10.6.0,<11.0.0) ; extra == "tensorrt"
Requires-Dist: tiktoken (>=0.5.0) ; extra == "chunking" or extra == "all"
Requires-Dist: timm ; extra == "vision" or extra == "all"
Requires-Dist: torch (>=2.2.1) ; extra == "ct2" or extra == "torch" or extra == "all"
Requires-Dist: torchvision ; extra == "vision" or extra == "all"
Requires-Dist: transformers[sentencepiece] (>=4.47.0,<=5.0) ; extra == "ct2"
Requires-Dist: typer (>=0.20.0,<0.21.0) ; extra == "server" or extra == "all"
Requires-Dist: uvicorn[standard] (>=0.32.0,<0.33.0) ; extra == "server" or extra == "all"
Project-URL: Homepage, https://github.com/michaelfeil/infinity
Project-URL: Repository, https://github.com/michaelfeil/infinity
Description-Content-Type: text/markdown

# Latent Bud

**High-Performance Embedding Server with Advanced Continuous Batching**

[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)
[![Python 3.9+](https://img.shields.io/badge/python-3.9+-blue.svg)](https://www.python.org/downloads/)

> Extended from [Infinity](https://github.com/michaelfeil/infinity) - A high-throughput, low-latency REST API for serving text-embeddings, reranking models, CLIP, CLAP and ColPali.

**Author:** Jithin VG, Bud Team

---

## Table of Contents

- [Overview](#overview)
- [Features](#features)
- [Architecture](#architecture)
- [Installation](#installation)
- [Quick Start](#quick-start)
- [Configuration](#configuration)
  - [CLI Reference](#cli-reference)
  - [Environment Variables](#environment-variables)
- [Token-Budget Continuous Batching](#token-budget-continuous-batching)
- [Priority Scheduling](#priority-scheduling)
- [GPU Optimizations](#gpu-optimizations)
- [Multi-Modal Support](#multi-modal-support)
- [Text Chunking](#text-chunking-with-chonkie)
- [Multi-Model Serving](#multi-model-serving)
- [Plugin System](#plugin-system)
- [Quantization](#quantization)
- [Python API](#python-api)
- [API Endpoints](#api-endpoints)
- [gRPC API](#grpc-api)
- [Performance Benchmarks](#performance-benchmarks)
- [Example Configurations](#example-configurations)
- [Observability](#observability)
- [Development](#development)
- [License](#license)

---

## Overview

Latent Bud is an enhanced embedding server built on top of the Infinity framework, featuring advanced token-budget continuous batching, GPU optimizations, plugin architecture, and performance optimizations that deliver up to **67% throughput improvement** for variable-length workloads.

### Key Highlights

- **Token-Budget Batching**: Maximize GPU utilization with padded-token-aware batching
- **Multi-Modal**: Text, Image (CLIP, SigLIP, ColPali), Audio (CLAP)
- **Plugin Architecture**: Extensible preprocessing, postprocessing, caching, and API hooks
- **OpenAI-Compatible**: Drop-in replacement for OpenAI embeddings API
- **Multi-Backend**: PyTorch, ONNX/TensorRT, CTranslate2, AWS Neuron
- **Production-Ready**: Prometheus metrics, OpenTelemetry, health checks

---

## Features

### Core Features

| Category | Features |
|----------|----------|
| **Models** | Any HuggingFace embedding, reranking, CLIP, CLAP, ColPali, SigLIP model |
| **Backends** | PyTorch (CUDA, ROCm, MPS, CPU), ONNX/TensorRT, CTranslate2, AWS Neuron |
| **Multi-Modal** | Text embeddings, Image (CLIP), Audio (CLAP), Document (ColPali) |
| **API** | OpenAI-compatible, Cohere-compatible reranking, streaming support |
| **Batching** | Token-budget continuous batching, bucket-based queuing, dynamic sizing |
| **Priority** | Request-level priority scheduling (high/normal/low), admission control |
| **Caching** | Vector disk cache, in-memory LRU cache with TTL |
| **Monitoring** | Prometheus metrics, OpenTelemetry tracing, health checks |
| **Scaling** | Multi-model orchestration, multi-GPU support, horizontal scaling |

### Latent Bud Enhancements

| Enhancement | Improvement | Description |
|-------------|-------------|-------------|
| Token-Budget Batching | +67% throughput | Batch by padded tokens, not sequence count |
| BucketQueue | +10-30% efficiency | Sequence-length-aware batching |
| Adaptive CUDA Streams | +30-50% throughput | Auto-switching pipeline modes |
| FlashBERT Attention | +10-15x for long seqs | O(N) memory attention |
| Plugin System | Extensible | Custom preprocessing, caching, API hooks |
| Triton Kernels | Hardware-optimized | Fused pooling and normalization |
| Priority Scheduling | QoS control | Three-tier priority with admission control |

---

## Architecture

```
+------------------------------------------------------------------+
|                         FastAPI Server                            |
|     (OpenAI-Compatible API + OpenTelemetry + Prometheus)          |
+------------------------------------------------------------------+
|                       Plugin System                               |
|  +------------+ +------------+ +------------+ +------------+      |
|  |Preprocessor| |Postprocess | |   Cache    | |    API     |      |
|  |  Plugins   | |  Plugins   | |  Plugins   | |  Plugins   |      |
|  +------------+ +------------+ +------------+ +------------+      |
+------------------------------------------------------------------+
|                      AsyncEngineArray                             |
|              (Multi-Model Orchestration Layer)                    |
+------------------------------------------------------------------+
|                    AsyncEmbeddingEngine                           |
|                  (Per-Model Inference Engine)                     |
+------------------------------------------------------------------+
|                       BatchHandler                                |
|  +------------------------------------------------------------+  |
|  |  TieredPriorityQueue / TokenBudgetQueue / BucketQueue      |  |
|  |  - Request Priority Scheduling (high/normal/low)           |  |
|  |  - Admission Control (reject low-priority under load)      |  |
|  |  - Longest-First Sorting within priority tiers             |  |
|  |  - Padded Token Calculation                                |  |
|  |  - Adaptive Batch Formation                                |  |
|  +------------------------------------------------------------+  |
+------------------------------------------------------------------+
|                    GPU Optimizations                              |
|  +-------------+ +-------------+ +-------------+ +-------------+  |
|  | Adaptive    | | FlashBERT   | | Triton      | | Pinned      |  |
|  | CUDA Stream | | Attention   | | Kernels     | | Memory      |  |
|  +-------------+ +-------------+ +-------------+ +-------------+  |
+------------------------------------------------------------------+
|                    Model Backends                                 |
|  +----------+  +----------+  +----------+  +----------+           |
|  |  PyTorch |  |  Optimum |  |CTranslate|  |  Neuron  |           |
|  |  (torch) |  |(ONNX/TRT)|  |   (ct2)  |  |  (AWS)   |           |
|  +----------+  +----------+  +----------+  +----------+           |
+------------------------------------------------------------------+
```

---

## Installation

### Via pip

```bash
# Full installation with all features
pip install infinity-emb[all]

# Server only
pip install infinity-emb[server]

# With chunking support
pip install infinity-emb[server,chunking]

# With vision support (CLIP, ColPali)
pip install infinity-emb[server,vision]

# With audio support (CLAP)
pip install infinity-emb[server,audio]
```

### FlashAttention (Recommended)

```bash
# Requires CUDA 11.8+ and PyTorch 2.0+
pip install flash-attn --no-build-isolation
```

### Via Docker

```bash
docker run -it --gpus all \
  -p 7997:7997 \
  michaelf34/infinity:latest \
  v2 \
  --model-id BAAI/bge-small-en-v1.5 \
  --max-batch-tokens 16384 \
  --cuda-streams true \
  --port 7997
```

### Via Poetry (Development)

```bash
cd libs/infinity_emb
poetry install --extras all --with lint,test
```

---

## Quick Start

### Start the Server

```bash
# Basic usage
infinity_emb v2 --model-id BAAI/bge-small-en-v1.5

# With all optimizations (recommended for production)
infinity_emb v2 --model-id BAAI/bge-small-en-v1.5 \
  --max-batch-tokens 16384 \
  --batch-strategy auto \
  --cuda-streams true \
  --flash-attention true
```

### Make API Requests

```python
import requests

# Create embeddings
response = requests.post(
    "http://localhost:7997/embeddings",
    json={
        "input": ["Hello world", "How are you?"],
        "model": "BAAI/bge-small-en-v1.5"
    }
)

embeddings = response.json()["data"]
print(f"Embedding dimension: {len(embeddings[0]['embedding'])}")
```

### Using Python SDK

```python
from infinity_emb import AsyncEmbeddingEngine, EngineArgs
import asyncio

async def main():
    args = EngineArgs(
        model_name_or_path="BAAI/bge-small-en-v1.5",
        max_batch_tokens=16384,
        cuda_streams=True,
    )

    engine = AsyncEmbeddingEngine.from_args(args)

    async with engine:
        embeddings, usage = await engine.embed([
            "Hello world",
            "How are you?"
        ])
        print(f"Generated {len(embeddings)} embeddings")
        print(f"Token usage: {usage}")

asyncio.run(main())
```

---

## Configuration

### CLI Reference

#### Core Options

| Option | Description | Default | Env Variable |
|--------|-------------|---------|--------------|
| `--model-id` | HuggingFace model ID | michaelfeil/bge-small-en-v1.5 | `INFINITY_MODEL_ID` |
| `--batch-size` | Maximum batch size | 32 | `INFINITY_BATCH_SIZE` |
| `--revision` | Model revision/branch | - | `INFINITY_REVISION` |
| `--trust-remote-code` | Trust remote code | true | `INFINITY_TRUST_REMOTE_CODE` |
| `--served-model-name` | API model name alias | - | `INFINITY_SERVED_MODEL_NAME` |

#### Engine & Device Options

| Option | Description | Default | Env Variable |
|--------|-------------|---------|--------------|
| `--engine` | Backend: `torch`, `optimum`, `ctranslate2`, `neuron` | torch | `INFINITY_ENGINE` |
| `--device` | Device: `auto`, `cuda`, `cpu`, `mps`, `tensorrt`, `xla` | auto | `INFINITY_DEVICE` |
| `--device-id` | GPU device ID(s) | - | `INFINITY_DEVICE_ID` |
| `--dtype` | Model dtype: `auto`, `float16`, `float32`, `bfloat16`, `int8`, `fp8` | auto | `INFINITY_DTYPE` |
| `--embedding-dtype` | Output dtype: `float32`, `int8`, `uint8`, `binary`, `ubinary` | float32 | `INFINITY_EMBEDDING_DTYPE` |
| `--pooling-method` | Pooling: `auto`, `mean`, `cls`, `max` | auto | `INFINITY_POOLING_METHOD` |

#### Token-Budget Batching Options

| Option | Description | Default | Env Variable |
|--------|-------------|---------|--------------|
| `--max-batch-tokens` | Max tokens per batch (0=disabled) | 0 | `INFINITY_MAX_BATCH_TOKENS` |
| `--batch-strategy` | Strategy: `auto`, `tokens`, `size` | auto | `INFINITY_BATCH_STRATEGY` |
| `--use-bucket-queue` | Sequence-length-aware batching | false | `INFINITY_USE_BUCKET_QUEUE` |
| `--lengths-via-tokenize` | Use tokenizer for accurate lengths | false | `INFINITY_LENGTHS_VIA_TOKENIZE` |

#### Priority Scheduling Options

| Option | Description | Default | Env Variable |
|--------|-------------|---------|--------------|
| `--priority-enabled` | Enable request-level priority scheduling | true | `INFINITY_PRIORITY_ENABLED` |
| `--priority-admission-high` | Queue fill ratio threshold for high priority (0.0-1.0) | 1.0 | `INFINITY_PRIORITY_ADMISSION_HIGH` |
| `--priority-admission-normal` | Queue fill ratio threshold for normal priority (0.0-1.0) | 0.9 | `INFINITY_PRIORITY_ADMISSION_NORMAL` |
| `--priority-admission-low` | Queue fill ratio threshold for low priority (0.0-1.0) | 0.7 | `INFINITY_PRIORITY_ADMISSION_LOW` |

#### GPU Optimization Options

| Option | Description | Default | Env Variable |
|--------|-------------|---------|--------------|
| `--cuda-streams` | Enable adaptive CUDA streams | false | `INFINITY_CUDA_STREAMS` |
| `--cuda-streams-mode` | Mode: `adaptive`, `stream`, `direct`, `legacy` | adaptive | `INFINITY_CUDA_STREAMS_MODE` |
| `--cuda-streams-threshold` | RPS threshold for mode switching | 80.0 | `INFINITY_CUDA_STREAMS_THRESHOLD` |
| `--prefetch-pipeline` | Enable CPU-GPU prefetch pipeline | true | `INFINITY_PREFETCH_PIPELINE` |
| `--prefetch-buffer-size` | Number of prepared batches in buffer (2 = double-buffering) | 2 | `INFINITY_PREFETCH_BUFFER_SIZE` |
| `--prefetch-with-transfer` | Include GPU transfer in prefetch phase | true | `INFINITY_PREFETCH_WITH_TRANSFER` |
| `--flash-attention` | Enable FlashBERT attention | false | `INFINITY_FLASH_ATTENTION` |
| `--flash-attention-backend` | Backend: `auto`, `flash_attn_cuda`, `triton`, `sdpa` | auto | `INFINITY_FLASH_ATTENTION_BACKEND` |
| `--flash-attention-min-seq-len` | Min sequence length | 64 | `INFINITY_FLASH_ATTENTION_MIN_SEQ_LEN` |
| `--compile` | Enable torch.compile | false | `INFINITY_COMPILE` |
| `--bettertransformer` | Enable BetterTransformer | true | `INFINITY_BETTERTRANSFORMER` |

#### Server Options

| Option | Description | Default | Env Variable |
|--------|-------------|---------|--------------|
| `--host` | Server host | 0.0.0.0 | `INFINITY_HOST` |
| `--port` | Server port | 7997 | `INFINITY_PORT` |
| `--url-prefix` | URL prefix for routes | - | `INFINITY_URL_PREFIX` |
| `--proxy-root-path` | Proxy root path | - | `INFINITY_PROXY_ROOT_PATH` |
| `--api-key` | API key for authentication | - | `INFINITY_API_KEY` |
| `--permissive-cors` | Allow permissive CORS | false | `INFINITY_PERMISSIVE_CORS` |
| `--log-level` | Log level | info | `INFINITY_LOG_LEVEL` |

#### Other Options

| Option | Description | Default | Env Variable |
|--------|-------------|---------|--------------|
| `--model-warmup` | Warmup model on startup | true | `INFINITY_MODEL_WARMUP` |
| `--vector-disk-cache` | Path for disk cache | - | `INFINITY_VECTOR_DISK_CACHE` |
| `--preload-only` | Only preload model, then exit | false | `INFINITY_PRELOAD_ONLY` |
| `--queue-size` | Max queue size | 32000 | `INFINITY_QUEUE_SIZE` |
| `--max-client-batch-size` | Max client batch size | 2048 | `INFINITY_MAX_CLIENT_BATCH_SIZE` |

### Environment Variables

All CLI options can be configured via environment variables with `INFINITY_` prefix:

```bash
# Model configuration
export INFINITY_MODEL_ID="BAAI/bge-small-en-v1.5"
export INFINITY_ENGINE="torch"
export INFINITY_DEVICE="cuda"
export INFINITY_DTYPE="float16"
export INFINITY_EMBEDDING_DTYPE="float32"

# Token-budget batching
export INFINITY_MAX_BATCH_TOKENS="16384"
export INFINITY_BATCH_STRATEGY="auto"
export INFINITY_BATCH_SIZE="64"
export INFINITY_USE_BUCKET_QUEUE="true"

# GPU optimizations
export INFINITY_CUDA_STREAMS="true"
export INFINITY_CUDA_STREAMS_MODE="adaptive"
export INFINITY_FLASH_ATTENTION="true"
export INFINITY_BETTERTRANSFORMER="true"

# CPU-GPU prefetch pipeline
export INFINITY_PREFETCH_PIPELINE="true"
export INFINITY_PREFETCH_BUFFER_SIZE="2"
export INFINITY_PREFETCH_WITH_TRANSFER="true"

# Server
export INFINITY_HOST="0.0.0.0"
export INFINITY_PORT="7997"
export INFINITY_API_KEY="your-secret-key"
export INFINITY_LOG_LEVEL="info"

# Priority scheduling
export INFINITY_PRIORITY_ENABLED="true"
export INFINITY_PRIORITY_ADMISSION_HIGH="1.0"
export INFINITY_PRIORITY_ADMISSION_NORMAL="0.9"
export INFINITY_PRIORITY_ADMISSION_LOW="0.7"

# Telemetry
export INFINITY_ANONYMOUS_USAGE_STATS="false"
export INFINITY_OTEL_ENABLED="true"

# Start server
infinity_emb v2
```

---

## Token-Budget Continuous Batching

Revolutionary batching system that maximizes GPU utilization by batching based on total padded tokens rather than sequence count.

### How It Works

```
Traditional Batching (batch_size=8):
┌──────────────────────────────────┐
│ Seq1: ████████                   │  128 tokens
│ Seq2: ██████████████████████████ │  512 tokens
│ Seq3: ████                       │   32 tokens
│ ...                              │
│ (All padded to 512 = 4096 tokens)│
└──────────────────────────────────┘

Token-Budget Batching (max_batch_tokens=2048):
┌──────────────────────────────────┐
│ Batch 1: Long sequences          │
│ Seq2: ██████████████████████████ │  512 tokens
│ Seq5: ████████████████████████   │  480 tokens
│ Seq8: ████████████████████       │  400 tokens
│ (Padded to 512 = 1536 tokens)    │
├──────────────────────────────────┤
│ Batch 2: Short sequences         │
│ Seq1: ████████                   │  128 tokens
│ Seq3: ████                       │   32 tokens
│ Seq4: ██████                     │   64 tokens
│ ... (many more fit!)             │
│ (Padded to 128 = 2048 tokens)    │
└──────────────────────────────────┘
```

### Enabling Token-Budget Batching

```bash
infinity_emb v2 --model-id BAAI/bge-small-en-v1.5 \
  --max-batch-tokens 16384 \
  --batch-strategy auto
```

### Batch Strategies

| Strategy | Description | Best For |
|----------|-------------|----------|
| `auto` | Token-budget when enabled, else size-based | Default |
| `tokens` | Always use token-budget batching | Variable-length workloads |
| `size` | Always use fixed batch size | Uniform-length workloads |

### Performance Impact

| Workload Type | Throughput Improvement |
|---------------|------------------------|
| Uniform Short Sequences | +118% |
| Uniform Medium Sequences | +99% |
| Uniform Long Sequences | +33% |
| Variable Length (Mixed) | +45% |
| Average | **+67%** |

---

## Priority Scheduling

Request-level priority scheduling enables Quality of Service (QoS) control by processing high-priority requests first and implementing admission control during queue overload.

### How It Works

```
Incoming Requests:
┌─────────────────────────────────────────────────────────┐
│  High Priority     Normal Priority     Low Priority     │
│  (Paid/Premium)    (Standard)          (Background)     │
└──────────┬─────────────┬─────────────────┬──────────────┘
           │             │                 │
           ▼             ▼                 ▼
┌─────────────────────────────────────────────────────────┐
│              TieredPriorityQueue                        │
│  ┌───────────────────────────────────────────────────┐  │
│  │ Tier 0 (High):   [req1, req5, req9]              │  │
│  │ Tier 1 (Normal): [req2, req3, req6, req7, req10] │  │
│  │ Tier 2 (Low):    [req4, req8]                    │  │
│  └───────────────────────────────────────────────────┘  │
│                                                         │
│  Batch Formation: High → Normal → Low                   │
│  (Longest-first within each tier)                       │
└─────────────────────────────────────────────────────────┘
           │
           ▼
┌─────────────────────────────────────────────────────────┐
│  Admission Control (during overload):                   │
│  - High:   Admitted up to 100% queue capacity           │
│  - Normal: Rejected when queue > 90% full               │
│  - Low:    Rejected when queue > 70% full               │
└─────────────────────────────────────────────────────────┘
```

### Priority Tiers

| Tier | Numeric Value | Admission Threshold | Use Case |
|------|---------------|---------------------|----------|
| `high` | 0 (highest) | 100% (always admitted) | Paid users, critical requests |
| `normal` | 1 | 90% queue fill | Standard requests (default) |
| `low` | 2 (lowest) | 70% queue fill | Background jobs, batch processing |

### Enabling Priority Scheduling

```bash
# Priority is enabled by default
infinity_emb v2 --model-id BAAI/bge-small-en-v1.5

# Disable priority (for backwards compatibility)
infinity_emb v2 --model-id BAAI/bge-small-en-v1.5 --priority-enabled false

# Custom admission thresholds
infinity_emb v2 --model-id BAAI/bge-small-en-v1.5 \
  --priority-admission-high 1.0 \
  --priority-admission-normal 0.85 \
  --priority-admission-low 0.6
```

### API Usage

#### Via Request Body

```python
import requests

# High priority request (e.g., paid user)
response = requests.post(
    "http://localhost:7997/embeddings",
    json={
        "input": ["Critical query that needs fast processing"],
        "model": "BAAI/bge-small-en-v1.5",
        "priority": "high"  # Will be processed first
    }
)

# Normal priority (default)
response = requests.post(
    "http://localhost:7997/embeddings",
    json={
        "input": ["Standard user query"],
        "model": "BAAI/bge-small-en-v1.5",
        "priority": "normal"  # Or omit for default
    }
)

# Low priority (background job)
response = requests.post(
    "http://localhost:7997/embeddings",
    json={
        "input": ["Batch processing item 1", "Batch processing item 2"],
        "model": "BAAI/bge-small-en-v1.5",
        "priority": "low"  # Processed last, may be rejected under load
    }
)
```

#### Via HTTP Header

```python
import requests

# Set priority via X-Priority header
response = requests.post(
    "http://localhost:7997/embeddings",
    headers={"X-Priority": "high"},
    json={
        "input": ["Important query"],
        "model": "BAAI/bge-small-en-v1.5"
    }
)

# Works with all endpoints
response = requests.post(
    "http://localhost:7997/rerank",
    headers={"X-Priority": "high"},
    json={
        "query": "What is machine learning?",
        "documents": ["ML is...", "The weather..."],
        "model": "BAAI/bge-reranker-base"
    }
)
```

### Admission Control Responses

When a request is rejected due to queue overload:

```http
HTTP/1.1 429 Too Many Requests
Content-Type: application/json

{
    "error": {
        "message": "Queue admission rejected: low priority requests not admitted when queue is 75% full",
        "type": "queue_overload",
        "code": 429
    }
}
```

### Python SDK Usage

```python
from infinity_emb import AsyncEmbeddingEngine, EngineArgs
from infinity_emb.primitives import PriorityTier
import asyncio

async def main():
    engine = AsyncEmbeddingEngine.from_args(
        EngineArgs(
            model_name_or_path="BAAI/bge-small-en-v1.5",
            priority_enabled=True,
        )
    )

    async with engine:
        # High priority embedding
        embeddings, usage = await engine.embed(
            sentences=["Critical query"],
            priority=PriorityTier.high
        )

        # Check if a request would be admitted
        admitted, reason = engine.check_admission(PriorityTier.low)
        if not admitted:
            print(f"Low priority would be rejected: {reason}")

asyncio.run(main())
```

### Configuration via Environment Variables

```bash
# Enable/disable priority scheduling
export INFINITY_PRIORITY_ENABLED=true

# Customize admission thresholds
export INFINITY_PRIORITY_ADMISSION_HIGH=1.0    # Always admit
export INFINITY_PRIORITY_ADMISSION_NORMAL=0.9  # Admit until 90% full
export INFINITY_PRIORITY_ADMISSION_LOW=0.7     # Admit until 70% full

infinity_emb v2 --model-id BAAI/bge-small-en-v1.5
```

### Performance Overhead

The priority system adds minimal overhead:

| Operation | Latency |
|-----------|---------|
| Priority extraction | ~0.1-0.2μs |
| Admission check | ~0.5-2μs |
| Total per-request overhead | ~1-3μs |

For a typical embedding request taking 5-50ms, this represents **0.01-0.05%** overhead.

### Use Cases

| Use Case | Configuration |
|----------|---------------|
| **SaaS with paid tiers** | high=paid, normal=free, low=trial |
| **Real-time + batch** | high=real-time, low=batch jobs |
| **Critical vs background** | high=user-facing, low=analytics |
| **Rate limiting** | Lower admission thresholds during peak |

---

## GPU Optimizations

### Adaptive CUDA Streams Pipeline

Intelligent multi-stream pipelining that automatically switches between stream-based and direct transfer modes.

```bash
infinity_emb v2 --model-id BAAI/bge-small-en-v1.5 \
  --cuda-streams true \
  --cuda-streams-mode adaptive \
  --cuda-streams-threshold 80.0
```

| Mode | Description | Best For |
|------|-------------|----------|
| `adaptive` | Auto-switches based on throughput | Variable workloads |
| `stream` | Always use CUDA streams | Low concurrency (<50 RPS) |
| `direct` | Always use direct transfers | High concurrency (>100 RPS) |
| `legacy` | Original implementation | Testing/debugging |

**Benefits:**
- 30-50% throughput improvement at low concurrency
- No overhead penalty at high concurrency
- Self-tuning based on workload characteristics

### FlashBERT Attention

Optimized attention for BERT-family models with O(N) memory complexity.

```bash
infinity_emb v2 --model-id BAAI/bge-small-en-v1.5 \
  --flash-attention true \
  --flash-attention-backend auto
```

| Backend | Description | Requirements |
|---------|-------------|--------------|
| `auto` | Auto-select best available | - |
| `flash_attn_cuda` | Official Flash Attention | `flash-attn` package |
| `triton` | Triton kernel implementation | Triton |
| `sdpa` | PyTorch SDPA | PyTorch 2.0+ |

**Benefits:**
- O(N) memory vs O(N²) standard attention
- 10-15x speedup for sequences >1024 tokens
- Works on consumer GPUs (RTX 20xx/30xx/40xx)

### torch.compile with CUDA Graphs

```bash
infinity_emb v2 --model-id BAAI/bge-small-en-v1.5 \
  --compile true
```

**Benefits:**
- Reduced kernel launch overhead
- Optimized memory access patterns
- Best for fixed batch sizes

---

## Multi-Modal Support

### Image Embeddings (CLIP, SigLIP, ColPali)

```bash
# Start server with vision model
infinity_emb v2 --model-id openai/clip-vit-base-patch32

# Or ColPali for document understanding
infinity_emb v2 --model-id vidore/colpali-v1.2
```

```python
import requests

# Embed image from URL
response = requests.post(
    "http://localhost:7997/embeddings",
    json={
        "model": "openai/clip-vit-base-patch32",
        "input": ["http://example.com/image.jpg"],
        "modality": "image"
    }
)

# Or embed from base64
response = requests.post(
    "http://localhost:7997/embeddings",
    json={
        "model": "openai/clip-vit-base-patch32",
        "input": ["data:image/jpeg;base64,/9j/4AAQSkZJRg..."],
        "modality": "image"
    }
)
```

### Audio Embeddings (CLAP)

```bash
# Start server with audio model
infinity_emb v2 --model-id laion/larger_clap_general
```

```python
import requests

# Embed audio from URL
response = requests.post(
    "http://localhost:7997/embeddings",
    json={
        "model": "laion/larger_clap_general",
        "input": ["http://example.com/audio.wav"],
        "modality": "audio"
    }
)

# Or text for audio search
response = requests.post(
    "http://localhost:7997/embeddings",
    json={
        "model": "laion/larger_clap_general",
        "input": ["sound of a dog barking"],
        "modality": "text"
    }
)
```

### Supported Multi-Modal Models

| Type | Models |
|------|--------|
| **Image** | CLIP (openai/clip-*), SigLIP, ColPali, ColIdefics2, ColQwen2 |
| **Audio** | CLAP (laion/larger_clap_general, laion/clap-htsat-*) |
| **Document** | ColPali (vidore/colpali-*) |

---

## Text Chunking with Chonkie

Built-in text chunking powered by the [Chonkie](https://github.com/chonkie-ai/chonkie) library.

### Basic Usage

```python
import requests

response = requests.post(
    "http://localhost:7997/embeddings",
    json={
        "input": ["Very long document text..." * 100],
        "model": "BAAI/bge-small-en-v1.5",
        "chunking": {
            "enabled": True,
            "strategy": "token",
            "chunk_size": 512,
            "chunk_overlap": 50
        }
    }
)

result = response.json()
print(f"Total chunks: {result['chunking_info']['total_chunks']}")
for chunk in result['data']:
    print(f"Chunk {chunk['chunk_info']['chunk_index']}: {len(chunk['embedding'])} dims")
```

### Chunking Strategies

| Strategy | Description | Best For |
|----------|-------------|----------|
| `token` | Fixed token count chunks | General use, precise control |
| `sentence` | Sentence-based boundaries | Natural text, readability |
| `recursive` | Hierarchical splitting | Structured documents, markdown |
| `semantic` | Meaning-based grouping | Topic coherence |
| `code` | Programming language aware | Source code |
| `table` | Table-aware chunking | Tabular data |

### Advanced Chunking Options

```python
{
    "chunking": {
        "enabled": True,
        "strategy": "semantic",
        "chunk_size": 512,
        "chunk_overlap": 50,
        "tokenizer": "cl100k_base",
        "semantic_threshold": 0.5,
        "chef": "markdown",          # Preprocessing: text, markdown, table
        "pipeline": ["sentence", "token"],  # Chain multiple chunkers
        "add_overlap_context": True,
        "overlap_size": 0.1,         # Fraction or absolute tokens
        "return_chunk_text": True
    }
}
```

---

## Multi-Model Serving

Serve multiple models simultaneously with independent configurations.

### Via CLI

```bash
infinity_emb v2 \
  --model-id BAAI/bge-small-en-v1.5 \
  --model-id BAAI/bge-large-en-v1.5 \
  --model-id openai/clip-vit-base-patch32 \
  --batch-size 64 --batch-size 32 --batch-size 16 \
  --max-batch-tokens 16384 --max-batch-tokens 8192 --max-batch-tokens 0
```

### Via Environment Variables

```bash
export INFINITY_MODEL_ID="BAAI/bge-small-en-v1.5;BAAI/bge-large-en-v1.5"
export INFINITY_BATCH_SIZE="64;32"
export INFINITY_MAX_BATCH_TOKENS="16384;8192"
infinity_emb v2
```

### Via Python API

```python
from infinity_emb import AsyncEngineArray, EngineArgs

engines = AsyncEngineArray.from_args([
    EngineArgs(
        model_name_or_path="BAAI/bge-small-en-v1.5",
        max_batch_tokens=16384,
    ),
    EngineArgs(
        model_name_or_path="openai/clip-vit-base-patch32",
        batch_size=16,
    ),
])

async with engines:
    # Text embeddings
    text_emb, _ = await engines[0].embed(["Hello world"])

    # Image embeddings
    img_emb, _ = await engines[1].image_embed(["http://example.com/image.jpg"])
```

---

## Plugin System

Latent Bud features an extensible plugin architecture for customizing preprocessing, postprocessing, caching, and API behavior.

### Plugin Types

| Type | Description | Use Cases |
|------|-------------|-----------|
| `PREPROCESSOR` | Input preprocessing hooks | Text normalization, filtering |
| `POSTPROCESSOR` | Output postprocessing hooks | Normalization, dimension reduction |
| `CACHE` | Alternative caching backends | Redis, Memcached integration |
| `SCHEDULER` | Batch scheduling strategies | Priority queues, rate limiting |
| `BACKEND` | Custom inference engines | TensorRT, OpenVINO |
| `HARDWARE` | Device detection/optimization | Custom hardware support |
| `API` | Custom endpoints, middleware | Authentication, logging |

### Built-in Plugins

| Plugin | Type | Description |
|--------|------|-------------|
| `MemoryCachePlugin` | CACHE | In-memory LRU cache with TTL |
| `EmbeddingNormalizer` | POSTPROCESSOR | L2 normalization |
| `TokenScheduler` | SCHEDULER | Token-budget aware scheduling |
| `TextNormalizer` | PREPROCESSOR | Input text normalization |
| `HealthAPI` | API | Custom health endpoints |

### Creating a Custom Plugin

#### Step 1: Create Plugin Class

```python
# my_plugins/custom_cache.py
from typing import Dict, Any, Optional
from infinity_emb.plugins.protocol import InfinityPlugin, PluginInfo, PluginType

class RedisCachePlugin(InfinityPlugin):
    """Redis-based caching plugin."""

    def __init__(self):
        self._redis = None
        self._ttl = 3600

    @property
    def info(self) -> PluginInfo:
        return PluginInfo(
            name="redis_cache",
            version="1.0.0",
            plugin_type=PluginType.CACHE,
            priority=30,  # Lower = higher priority
            description="Redis-based embedding cache",
            author="Your Name",
        )

    async def initialize(self, config: Dict[str, Any]) -> None:
        """Initialize Redis connection."""
        import redis.asyncio as redis

        self._redis = redis.Redis(
            host=config.get("host", "localhost"),
            port=config.get("port", 6379),
            db=config.get("db", 0),
        )
        self._ttl = config.get("ttl", 3600)

    async def shutdown(self) -> None:
        """Close Redis connection."""
        if self._redis:
            await self._redis.close()

    def health_check(self) -> Dict[str, Any]:
        """Return health status."""
        return {
            "status": "healthy" if self._redis else "not_initialized",
            "type": "redis_cache",
        }

    # Cache-specific methods
    async def get(self, key: str) -> Optional[bytes]:
        """Get cached embedding."""
        return await self._redis.get(key)

    async def set(self, key: str, value: bytes) -> None:
        """Store embedding in cache."""
        await self._redis.setex(key, self._ttl, value)
```

#### Step 2: Create Preprocessor Plugin

```python
# my_plugins/text_cleaner.py
from typing import Dict, Any, List
from infinity_emb.plugins.protocol import InfinityPlugin, PluginInfo, PluginType

class TextCleanerPlugin(InfinityPlugin):
    """Text preprocessing plugin."""

    def __init__(self):
        self._lowercase = False
        self._strip_html = False

    @property
    def info(self) -> PluginInfo:
        return PluginInfo(
            name="text_cleaner",
            version="1.0.0",
            plugin_type=PluginType.PREPROCESSOR,
            priority=10,
        )

    async def initialize(self, config: Dict[str, Any]) -> None:
        self._lowercase = config.get("lowercase", False)
        self._strip_html = config.get("strip_html", False)

    async def shutdown(self) -> None:
        pass

    def health_check(self) -> Dict[str, Any]:
        return {"status": "healthy", "type": "preprocessor"}

    def preprocess(self, texts: List[str]) -> List[str]:
        """Clean input texts."""
        import re

        cleaned = []
        for text in texts:
            if self._strip_html:
                text = re.sub(r'<[^>]+>', '', text)
            if self._lowercase:
                text = text.lower()
            text = ' '.join(text.split())  # Normalize whitespace
            cleaned.append(text)
        return cleaned
```

#### Step 3: Register Plugin via Entry Points

Add to your `pyproject.toml`:

```toml
[project.entry-points."infinity_emb.plugins"]
redis_cache = "my_plugins.custom_cache:RedisCachePlugin"
text_cleaner = "my_plugins.text_cleaner:TextCleanerPlugin"
```

Or `setup.py`:

```python
setup(
    ...
    entry_points={
        "infinity_emb.plugins": [
            "redis_cache = my_plugins.custom_cache:RedisCachePlugin",
            "text_cleaner = my_plugins.text_cleaner:TextCleanerPlugin",
        ]
    }
)
```

#### Step 4: Configure and Use

```python
from infinity_emb import AsyncEmbeddingEngine, EngineArgs
from infinity_emb.plugins.registry import PluginRegistry

# Discover and load plugins
registry = PluginRegistry()
registry.discover_plugins()

# Configure plugins
plugin_config = {
    "redis_cache": {
        "host": "localhost",
        "port": 6379,
        "ttl": 7200,
    },
    "text_cleaner": {
        "lowercase": True,
        "strip_html": True,
    }
}

# Initialize plugins
await registry.initialize_all(plugin_config)

# Use with engine
engine = AsyncEmbeddingEngine.from_args(
    EngineArgs(model_name_or_path="BAAI/bge-small-en-v1.5")
)
```

### Plugin Configuration File

Create `infinity_plugins.yaml`:

```yaml
plugins:
  redis_cache:
    enabled: true
    host: localhost
    port: 6379
    ttl: 3600

  text_cleaner:
    enabled: true
    lowercase: false
    strip_html: true

  embedding_normalizer:
    enabled: true

  token_scheduler:
    enabled: true
    max_batch_tokens: 16384
```

### Plugin Lifecycle

```
1. Discovery: Plugins found via setuptools entry points
2. Validation: Plugin compatibility checked
3. Dependency Resolution: Plugin order determined by priority
4. Initialization: async initialize() called with config
5. Runtime: Plugin hooks called during request processing
6. Shutdown: async shutdown() called on server stop
```

---

## Quantization

### Model Weight Quantization

```bash
# Float16 (half precision)
infinity_emb v2 --model-id BAAI/bge-small-en-v1.5 --dtype float16

# BFloat16 (brain float)
infinity_emb v2 --model-id BAAI/bge-small-en-v1.5 --dtype bfloat16

# Int8 quantization
infinity_emb v2 --model-id BAAI/bge-small-en-v1.5 --dtype int8

# FP8 (8-bit floating point)
infinity_emb v2 --model-id BAAI/bge-small-en-v1.5 --dtype fp8
```

### Embedding Output Quantization

```bash
# Int8 output embeddings
infinity_emb v2 --model-id BAAI/bge-small-en-v1.5 --embedding-dtype int8

# Binary quantization (1-bit)
infinity_emb v2 --model-id BAAI/bge-small-en-v1.5 --embedding-dtype binary

# Unsigned binary
infinity_emb v2 --model-id BAAI/bge-small-en-v1.5 --embedding-dtype ubinary
```

### Via API (Matryoshka)

```python
import requests

# Request reduced dimensions (Matryoshka)
response = requests.post(
    "http://localhost:7997/embeddings",
    json={
        "input": ["Hello world"],
        "model": "BAAI/bge-small-en-v1.5",
        "dimensions": 256  # Reduce from 384 to 256
    }
)
```

---

## Python API

### Basic Usage

```python
from infinity_emb import AsyncEmbeddingEngine, EngineArgs
import asyncio

async def main():
    # Create engine with configuration
    engine = AsyncEmbeddingEngine.from_args(
        EngineArgs(
            model_name_or_path="BAAI/bge-small-en-v1.5",
            batch_size=64,
            max_batch_tokens=16384,
            device="cuda",
            dtype="float16",
        )
    )

    async with engine:
        # Text embeddings
        embeddings, usage = await engine.embed([
            "Hello world",
            "How are you?"
        ])
        print(f"Embeddings: {len(embeddings)}, Usage: {usage}")

        # With matryoshka dimension
        embeddings, usage = await engine.embed(
            sentences=["Hello world"],
            matryoshka_dim=256
        )

asyncio.run(main())
```

### Multi-Model with AsyncEngineArray

```python
from infinity_emb import AsyncEngineArray, EngineArgs

async def main():
    engines = AsyncEngineArray.from_args([
        EngineArgs(model_name_or_path="BAAI/bge-small-en-v1.5"),
        EngineArgs(model_name_or_path="sentence-transformers/all-MiniLM-L6-v2"),
    ])

    async with engines:
        # Access by index
        emb1, _ = await engines[0].embed(["text"])
        emb2, _ = await engines[1].embed(["text"])

        # Or iterate
        for engine in engines:
            emb, _ = await engine.embed(["text"])

asyncio.run(main())
```

### Reranking

```python
async def rerank_example():
    engine = AsyncEmbeddingEngine.from_args(
        EngineArgs(model_name_or_path="BAAI/bge-reranker-base")
    )

    async with engine:
        scores, usage = await engine.rerank(
            query="What is machine learning?",
            documents=[
                "Machine learning is a subset of AI.",
                "The weather is nice today.",
                "Deep learning uses neural networks.",
            ]
        )
        print(f"Scores: {scores}")

asyncio.run(rerank_example())
```

### Image Embeddings

```python
async def image_example():
    engine = AsyncEmbeddingEngine.from_args(
        EngineArgs(model_name_or_path="openai/clip-vit-base-patch32")
    )

    async with engine:
        # From URLs
        embeddings, usage = await engine.image_embed([
            "http://example.com/image1.jpg",
            "http://example.com/image2.jpg",
        ])

        # Text for image search
        text_emb, _ = await engine.embed(["a photo of a cat"])

asyncio.run(image_example())
```

### Creating a Server Programmatically

```python
from infinity_emb import create_server, EngineArgs
import uvicorn

app = create_server(
    engine_args_list=[
        EngineArgs(
            model_name_or_path="BAAI/bge-small-en-v1.5",
            max_batch_tokens=16384,
        )
    ],
    url_prefix="/v1",
    api_key="your-secret-key",
)

if __name__ == "__main__":
    uvicorn.run(app, host="0.0.0.0", port=7997)
```

---

## API Endpoints

### Embeddings (OpenAI-Compatible)

```http
POST /embeddings
Content-Type: application/json
X-Priority: normal

{
    "input": ["text1", "text2"],
    "model": "BAAI/bge-small-en-v1.5",
    "encoding_format": "float",  // or "base64"
    "dimensions": 384,           // optional matryoshka
    "modality": "text",          // "text", "image", "audio"
    "priority": "normal",        // "high", "normal", "low" (or use X-Priority header)
    "chunking": {                // optional
        "enabled": true,
        "strategy": "token",
        "chunk_size": 512
    }
}
```

Response:
```json
{
    "object": "list",
    "data": [
        {"object": "embedding", "embedding": [...], "index": 0}
    ],
    "model": "BAAI/bge-small-en-v1.5",
    "usage": {"prompt_tokens": 10, "total_tokens": 10}
}
```

### Reranking

```http
POST /rerank
Content-Type: application/json

{
    "model": "BAAI/bge-reranker-base",
    "query": "What is AI?",
    "documents": ["AI is...", "The weather..."],
    "top_n": 3,
    "return_documents": true
}
```

### Classification

```http
POST /classify
Content-Type: application/json

{
    "model": "SamLowe/roberta-base-go_emotions",
    "input": ["I am so happy today!"]
}
```

### Models

```http
GET /models
```

Response:
```json
{
    "data": [
        {
            "id": "BAAI/bge-small-en-v1.5",
            "object": "model",
            "capabilities": ["embed"],
            "stats": {"queue_size": 0, "batch_size": 32}
        }
    ]
}
```

### Health

```http
GET /health
```

Response:
```json
{
    "status": "healthy",
    "models": ["BAAI/bge-small-en-v1.5"],
    "unix": 1699999999.0
}
```

### Metrics (Prometheus)

```http
GET /metrics
```

---

## gRPC API

Latent Bud includes a high-performance gRPC server that provides **5x faster throughput** and **75% lower latency** compared to REST.

### Installation

```bash
# Install with gRPC support
pip install infinity-emb[grpc]

# Or with all features
pip install infinity-emb[all]
```

### Enabling gRPC Server

```bash
# Start with both REST and gRPC
infinity_emb v2 --model-id BAAI/bge-small-en-v1.5 \
  --grpc-port 50051 \
  --grpc-reflection true

# With authentication (same API key for both REST and gRPC)
infinity_emb v2 --model-id BAAI/bge-small-en-v1.5 \
  --grpc-port 50051 \
  --api-key "your-secret-key"
```

### CLI Options

| Option | Description | Default | Env Variable |
|--------|-------------|---------|--------------|
| `--grpc-port` | gRPC server port (0 = disabled) | 0 | `INFINITY_GRPC_PORT` |
| `--grpc-reflection` | Enable gRPC reflection for debugging | true | `INFINITY_GRPC_REFLECTION` |

### gRPC Endpoints

The gRPC service implements the `infinity.v1.EmbeddingService` with these methods:

| Method | Description | Request | Response |
|--------|-------------|---------|----------|
| `Embed` | Text embeddings | `EmbedRequest` | `EmbedResponse` |
| `EmbedStream` | Streaming text embeddings | `stream EmbedRequest` | `stream EmbedResponse` |
| `Rerank` | Document reranking | `RerankRequest` | `RerankResponse` |
| `RerankStream` | Streaming reranking | `stream RerankRequest` | `stream RerankResponse` |
| `Classify` | Text classification | `ClassifyRequest` | `ClassifyResponse` |
| `ClassifyStream` | Streaming classification | `stream ClassifyRequest` | `stream ClassifyResponse` |
| `ImageEmbed` | Image embeddings | `ImageEmbedRequest` | `EmbedResponse` |
| `AudioEmbed` | Audio embeddings | `AudioEmbedRequest` | `EmbedResponse` |
| `Info` | Server information | `InfoRequest` | `InfoResponse` |
| `Health` | Health check | `HealthRequest` | `HealthResponse` |

### Python gRPC Client

```python
import grpc
import asyncio
from infinity_emb.grpc import infinity_pb2, infinity_pb2_grpc

async def main():
    # Connect to gRPC server
    async with grpc.aio.insecure_channel('localhost:50051') as channel:
        stub = infinity_pb2_grpc.EmbeddingServiceStub(channel)

        # Text embeddings
        request = infinity_pb2.EmbedRequest(
            inputs=["Hello, world!", "How are you?"],
            model="BAAI/bge-small-en-v1.5",
        )
        response = await stub.Embed(request)

        for emb in response.embeddings:
            print(f"Embedding {emb.index}: {len(emb.values)} dimensions")
        print(f"Usage: {response.usage.total_tokens} tokens")

        # Reranking
        rerank_request = infinity_pb2.RerankRequest(
            query="What is machine learning?",
            documents=[
                "Machine learning is a subset of AI.",
                "The weather is nice today.",
            ],
            model="BAAI/bge-reranker-base",
            top_n=2,
        )
        rerank_response = await stub.Rerank(rerank_request)

        for result in rerank_response.results:
            print(f"Doc {result.index}: score={result.relevance_score:.4f}")

        # Server info
        info = await stub.Info(infinity_pb2.InfoRequest())
        print(f"Server version: {info.version}")
        for model in info.models:
            print(f"  Model: {model.served_model_name} ({model.backend})")

asyncio.run(main())
```

### gRPC with Authentication

```python
import grpc
from infinity_emb.grpc import infinity_pb2, infinity_pb2_grpc

async def authenticated_request():
    # Add API key to metadata
    metadata = [('authorization', 'Bearer your-secret-key')]
    # Or: metadata = [('x-api-key', 'your-secret-key')]

    async with grpc.aio.insecure_channel('localhost:50051') as channel:
        stub = infinity_pb2_grpc.EmbeddingServiceStub(channel)

        request = infinity_pb2.EmbedRequest(
            inputs=["Hello world"],
            model="BAAI/bge-small-en-v1.5",
        )

        # Pass metadata with each call
        response = await stub.Embed(request, metadata=metadata)
        print(f"Embeddings: {len(response.embeddings)}")
```

### Streaming Requests

```python
import grpc
from infinity_emb.grpc import infinity_pb2, infinity_pb2_grpc

async def streaming_embed():
    async with grpc.aio.insecure_channel('localhost:50051') as channel:
        stub = infinity_pb2_grpc.EmbeddingServiceStub(channel)

        # Create request stream
        async def request_iterator():
            for batch in [["text1", "text2"], ["text3", "text4"]]:
                yield infinity_pb2.EmbedRequest(
                    inputs=batch,
                    model="BAAI/bge-small-en-v1.5",
                )

        # Process response stream
        async for response in stub.EmbedStream(request_iterator()):
            print(f"Batch: {len(response.embeddings)} embeddings")
```

### Testing with grpcurl

```bash
# Install grpcurl
brew install grpcurl  # macOS
# or: go install github.com/fullstorydev/grpcurl/cmd/grpcurl@latest

# List available services
grpcurl -plaintext localhost:50051 list

# Describe service methods
grpcurl -plaintext localhost:50051 describe infinity.v1.EmbeddingService

# Get server info
grpcurl -plaintext localhost:50051 infinity.v1.EmbeddingService/Info

# Health check
grpcurl -plaintext localhost:50051 infinity.v1.EmbeddingService/Health

# Embed text
grpcurl -plaintext -d '{
  "inputs": ["Hello world", "How are you?"],
  "model": "BAAI/bge-small-en-v1.5"
}' localhost:50051 infinity.v1.EmbeddingService/Embed

# Rerank documents
grpcurl -plaintext -d '{
  "query": "What is AI?",
  "documents": ["AI is artificial intelligence", "The sky is blue"],
  "model": "BAAI/bge-reranker-base",
  "top_n": 2
}' localhost:50051 infinity.v1.EmbeddingService/Rerank

# With authentication
grpcurl -plaintext \
  -H "authorization: Bearer your-secret-key" \
  -d '{"inputs": ["Hello"]}' \
  localhost:50051 infinity.v1.EmbeddingService/Embed
```

### Proto Schema

The proto definition is at `infinity_emb/grpc/proto/infinity.proto`:

```protobuf
syntax = "proto3";
package infinity.v1;

service EmbeddingService {
    rpc Embed(EmbedRequest) returns (EmbedResponse);
    rpc EmbedStream(stream EmbedRequest) returns (stream EmbedResponse);
    rpc Rerank(RerankRequest) returns (RerankResponse);
    rpc Classify(ClassifyRequest) returns (ClassifyResponse);
    rpc ImageEmbed(ImageEmbedRequest) returns (EmbedResponse);
    rpc AudioEmbed(AudioEmbedRequest) returns (EmbedResponse);
    rpc Info(InfoRequest) returns (InfoResponse);
    rpc Health(HealthRequest) returns (HealthResponse);
}

message EmbedRequest {
    repeated string inputs = 1;
    string model = 2;
    bool truncate = 3;
    bool normalize = 4;
    optional int32 dimensions = 5;  // Matryoshka dimensions
}

message EmbedResponse {
    repeated Embedding embeddings = 1;
    Usage usage = 2;
    Metadata metadata = 3;
    string model = 4;
}
```

### gRPC vs REST Performance

| Metric | gRPC | REST | Improvement |
|--------|------|------|-------------|
| **Avg Throughput** | 1,114 req/s | 213 req/s | **5.23x faster** |
| **Avg P50 Latency** | 27ms | 108ms | **75% lower** |
| **Avg P99 Latency** | 39ms | 167ms | **77% lower** |
| **Peak Throughput** | 2,549 req/s | 544 req/s | **4.7x faster** |

**Throughput by Concurrency (batch_size=1):**

| Concurrency | gRPC RPS | REST RPS | Speedup |
|-------------|----------|----------|---------|
| 1 | 545 | 93 | 5.9x |
| 4 | 2,549 | 251 | 10.1x |
| 8 | 2,539 | 443 | 5.7x |
| 16 | 2,396 | 467 | 5.1x |
| 32 | 2,394 | 544 | 4.4x |
| 64 | 2,496 | 452 | 5.5x |

### When to Use gRPC

**Use gRPC when:**
- High throughput is critical (>1000 req/s)
- Low latency required (<10ms P50)
- Batch processing with high concurrency
- Internal microservice communication
- Python/Go/Java/C++ clients

**Use REST when:**
- Browser clients (no native gRPC support)
- Simple integration requirements
- Debugging/curl testing needed
- OpenAPI tooling required

---

## Performance Benchmarks

### Token-Budget Batching Impact

| Workload Type | Throughput Improvement |
|---------------|------------------------|
| Uniform Short Sequences | +118.0% |
| Uniform Medium Sequences | +99.0% |
| Uniform Long Sequences | +33.2% |
| Variable Length (Mixed) | +45.4% |
| Large Batch Processing | +41.3% |
| **Average** | **+67%** |

### Adaptive Pipeline (BGE-small-en-v1.5, RTX GPU)

| Concurrency | Throughput (RPS) | Tokens/sec | P50 Latency |
|-------------|------------------|------------|-------------|
| 1           | 110.36           | 32,964     | 8.6ms       |
| 8           | 102.66           | 21,870     | 68ms        |
| 32          | 94.89            | 19,637     | 251ms       |
| 64          | 148.35           | 29,617     | 366ms       |
| 128         | 286.65           | 58,550     | 309ms       |
| 256         | 258.06           | 53,222     | 918ms       |

---

## Example Configurations

### High-Throughput Production

```bash
infinity_emb v2 --model-id BAAI/bge-small-en-v1.5 \
  --max-batch-tokens 16384 \
  --batch-size 64 \
  --use-bucket-queue true \
  --cuda-streams true \
  --cuda-streams-mode adaptive \
  --flash-attention true \
  --lengths-via-tokenize true \
  --dtype float16
```

### Low-Latency Real-Time

```bash
infinity_emb v2 --model-id BAAI/bge-small-en-v1.5 \
  --max-batch-tokens 4096 \
  --batch-size 16 \
  --cuda-streams true \
  --cuda-streams-mode stream
```

### Memory-Constrained

```bash
infinity_emb v2 --model-id BAAI/bge-small-en-v1.5 \
  --max-batch-tokens 2048 \
  --batch-size 8 \
  --dtype float16 \
  --flash-attention true
```

### CPU-Only Deployment

```bash
infinity_emb v2 --model-id BAAI/bge-small-en-v1.5 \
  --device cpu \
  --engine ctranslate2 \
  --batch-size 8
```

### Multi-Modal Server

```bash
infinity_emb v2 \
  --model-id BAAI/bge-small-en-v1.5 \
  --model-id openai/clip-vit-base-patch32 \
  --model-id laion/larger_clap_general \
  --batch-size 32 --batch-size 16 --batch-size 8
```

---

## Observability

Latent Bud provides comprehensive observability features including OpenTelemetry tracing, structured logging, and Prometheus metrics.

### OpenTelemetry Tracing

Enable distributed tracing to monitor request flow and performance.

```bash
# Enable OpenTelemetry
export INFINITY_OTEL_ENABLED=1

# Configure collector endpoint (default: localhost:4317)
export OTEL_EXPORTER_OTLP_ENDPOINT="http://localhost:4317"

# Set service name (default: infinity-emb)
export OTEL_SERVICE_NAME="my-embedding-service"

infinity_emb v2 --model-id BAAI/bge-small-en-v1.5
```

**Instrumented Operations:**
- `engine.embed` - Text embedding requests
- `engine.rerank` - Reranking requests
- `engine.classify` - Classification requests
- `engine.image_embed` - Image embedding requests
- `engine.audio_embed` - Audio embedding requests
- `batch_handler.schedule` - Batch scheduling operations
- HTTP requests (via FastAPI auto-instrumentation)

**Span Attributes:**
- `model.name` - Model being used
- `input.count` - Number of inputs in request
- `output.usage` - Token usage statistics

**OTEL Collector Setup (Docker Compose):**

```yaml
# docker-compose.yml
version: '3'
services:
  infinity:
    image: michaelf34/infinity:latest
    ports:
      - "7997:7997"
    environment:
      - INFINITY_OTEL_ENABLED=1
      - OTEL_EXPORTER_OTLP_ENDPOINT=http://jaeger:4317
      - OTEL_SERVICE_NAME=infinity-emb
    command: v2 --model-id BAAI/bge-small-en-v1.5

  jaeger:
    image: jaegertracing/all-in-one:latest
    ports:
      - "16686:16686"  # Jaeger UI
      - "4317:4317"    # OTLP gRPC
```

### Structured Logging

Enable JSON structured logging for log aggregation systems (ELK, Loki, etc.).

```bash
# Enable JSON logging
export INFINITY_LOG_JSON=1
export INFINITY_LOG_LEVEL=info

infinity_emb v2 --model-id BAAI/bge-small-en-v1.5
```

**Log Output Example:**
```json
{
  "timestamp": 1699999999.123,
  "level": "INFO",
  "name": "infinity_emb",
  "message": "model warmed up, 89248.87 embeddings/sec at batch_size=16",
  "service.name": "infinity-emb"
}
```

**Log Levels:**
| Level | Description |
|-------|-------------|
| `trace` | Most verbose, internal details |
| `debug` | Debugging information |
| `info` | General operational info (default) |
| `warning` | Warning messages |
| `error` | Error messages |
| `critical` | Critical failures |

### Prometheus Metrics

Built-in Prometheus metrics via FastAPI Instrumentator.

```bash
# Metrics endpoint
curl http://localhost:7997/metrics
```

**Available Metrics:**
- `http_requests_total` - Total HTTP requests by method, path, status
- `http_request_duration_seconds` - Request duration histogram
- `http_request_size_bytes` - Request size histogram
- `http_response_size_bytes` - Response size histogram
- `http_requests_in_progress` - Current in-flight requests

**Prometheus Scrape Config:**
```yaml
# prometheus.yml
scrape_configs:
  - job_name: 'infinity-emb'
    static_configs:
      - targets: ['localhost:7997']
    metrics_path: /metrics
    scrape_interval: 15s
```

### Environment Variables Summary

| Variable | Default | Description |
|----------|---------|-------------|
| `INFINITY_OTEL_ENABLED` | `0` | Enable OpenTelemetry tracing |
| `OTEL_EXPORTER_OTLP_ENDPOINT` | `localhost:4317` | OTLP collector endpoint |
| `OTEL_SERVICE_NAME` | `infinity-emb` | Service name for traces |
| `INFINITY_LOG_JSON` | `0` | Enable JSON structured logging |
| `INFINITY_LOG_LEVEL` | `info` | Log level |
| `INFINITY_ANONYMOUS_USAGE_STATS` | `true` | Anonymous telemetry (PostHog) |
| `DO_NOT_TRACK` | - | Set to `1` to disable all telemetry |

### Grafana Dashboard Example

```json
{
  "panels": [
    {
      "title": "Request Rate",
      "targets": [{
        "expr": "rate(http_requests_total{job=\"infinity-emb\"}[5m])"
      }]
    },
    {
      "title": "P99 Latency",
      "targets": [{
        "expr": "histogram_quantile(0.99, rate(http_request_duration_seconds_bucket{job=\"infinity-emb\"}[5m]))"
      }]
    },
    {
      "title": "Error Rate",
      "targets": [{
        "expr": "rate(http_requests_total{job=\"infinity-emb\",status=~\"5..\"}[5m])"
      }]
    }
  ]
}
```

---

## Development

### Running Tests

```bash
cd libs/infinity_emb
poetry install --extras all --with lint,test

# All tests
make test

# Specific tests
pytest tests/unit_test/inference/test_token_budget_batching.py -v
pytest tests/unit_test/transformer/test_dummytransformer_fixes.py -v
pytest tests/end_to_end/test_sentence_transformers.py -v
```

### Code Quality

```bash
make format    # Format code
make lint      # Run linting
make precommit # Full pre-commit checks
```

### Running Benchmarks

```bash
# Start server
infinity_emb v2 --model-id BAAI/bge-small-en-v1.5 \
  --max-batch-tokens 16384 \
  --cuda-streams true

# Run benchmark
python benchmark/benchmark.py \
  --server-url http://localhost:7997 \
  --concurrency 1,8,32,64,128
```

---

## License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

---

## Acknowledgments

- **Infinity** by Michael Feil - The foundation for this project
- **Text Embeddings Inference (TEI)** by Hugging Face - Inspiration for token-budget batching
- **FlashAttention** by Tri Dao - Efficient attention implementation
- **Triton** by OpenAI - Fused kernel development
- **Chonkie** - Text chunking library
- **vLLM** - Research on CPU overhead and adaptive batching
- **Bud Team** - Continuous batching implementation and GPU optimizations

---

## Contributing

Contributions are welcome! Please feel free to submit a Pull Request.

---

**Latent Bud** - High-Performance Embeddings at Scale

*By Jithin VG, Bud Team*

